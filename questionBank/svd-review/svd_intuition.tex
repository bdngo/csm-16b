% Author: Taejin Hwang

\qns{SVD Intuition (Optional)}

\meta{
  Prereqs: Spectral Theorem, Norms, and a strong understanding of eigenvectors
  This is a very difficult question that is at the level of EECS 127, and is on par with the difficulty of a homework question. However, the entire question can be done through with a 16B level of understanding of Linear Algebra, and will serve to be a strong foundation for the SVD and its connections with the Moore- Penrose inverse along with the Four Fundamental Subspaces of A.
  Remember that the SVD is seen as a generalization of diagonalization, for non-square and non-diagonalizable matrices. Since AT A and AAT are guaranteed to have full set of linearly independent eigenvectors, we use these eigenvectors to construct the SVD.
}

Now that we’ve taken a look at symmetric matrices, specifically $A^{T} A,$ we will investigate into the intuition behind the "Swiss-Army Knife" of Linear Algebra, the \textbf{Singular Value Decomposition.} You may have seen the following forms of the SVD of an m × n matrix A from lecture, homework, and discussion:
\begin{equation}
  A = U \Sigma V^{T} = \sum_{i=1}^{r} \sigma_{i} \vec{u}_{i} \vec{v}_{i}^T
\end{equation}
In the SVD, the $\sigma_{i}$ are strictly positive real numbers referred to as the \textbf{singular values}, $\vec{u}_{i}, \vec{v}_{i}$ are the $i^{th}$ column of orthonormal matrices $U$ and $V$ respectively, and $r = \operatorname{rank}(A)$.

An amazing result in Linear Algebra is the Spectral Theorem which says that any symmetric matrix is orthogonally diagonalizable.
This means that a \textit{symmetric matrix will always have $n$ linearly independent eigenvectors that are all mutually orthogonal.} In particular, we can choose the set to consist of unit eigenvectors with $\Vert \vec{v} \Vert = 1$, which makes it an \textit{orthonormal} set.

We will start our investigation by looking at the eigenvectors of the matrix $A^T A$, which we will first show is symmetric and has non-negative eigenvalues.

From here, we will walk you through a proof of the SVD.

\begin{enumerate}

  \qitem Let $A$ be a matrix in $\mathbb{R}^{m \times n}$. \textbf{Show that the matrices $A^T A$ and $A A^T$ are symmetric.}

  \textit{Hint: This should be a very quick proof, using the definition of a symmetric matrix $M^{T} = M$. }

  \ws{\vspace{50px}}
  \sol{
    We use the definition of a symmetric matrix $B$ having the property $B = B^T$.
    \begin{align*}
      (A^T A)^T = A^T (A^T)^T = A^T A \\
      (A A^T)^T = (A^T)^T A^T = A A^T
    \end{align*}
  }

  \qitem \textbf{Show that $A^T A$ has non-negative eigenvalues.} A similar proof applies for $A A^T$.

  \textit{Hint: Consider $\norm{A\vec{v}}_{2}^{2}.$ where $\vec{v}$ is an eigenvector of $A^{T}A$ with eigenvalue $\lambda.$ The subscript 2 just denotes the euclidean norm that's been used in class already.}

  \ws{\vspace{75px}}

  \sol {
    $\norm{A\vec{v}}_{2}^{2} = (A \vec{v})^{T} (A \vec{v}) = \vec{v}^{T}A^{T} A \vec{v} = \vec{v}^{T} (\lambda \vec{v}) = \lambda \vec{v}^{T} \vec{v} = \lambda \norm{\vec{v}}_{2}^{2}.$ \vskip 1pt
    Therefore we see that:
    \begin{equation}
      \lambda = \frac{\norm{A \vec{v}}_{2}^{2}}{\norm{\vec{v}}_{2}^{2}} \geq 0
    \end{equation}
    $\vec{v}$ is an eigenvector, so it must be nonzero, meaning its norm squared will be greater than 0. \vskip 1pt
    $\norm{A\vec{v}}_{2}^{2}$ will also be greater than or equal to zero, but may be zero if $A \vec{v} = 0$ or $\vec{v} \in \text{Nul}(A).$
  }
\end{enumerate}

Now that we've shown $A^T A$ and $A A^T$ to be symmetric with non-negative eigenvalues, we can continue our analysis to relate the eigenvectors of $A^T A$ with the eigenvectors of $A A^T$.

\begin{enumerate}[resume]
  \qitem In this part, we will show that the matrices $A^T A$ and $A A^T$ share eigenvalues and have a relationship between their eigenvectors.

  \begin{enumerate}
  \qitem
  Let $\vec{v}$ be a nonzero vector in $\mathbb{R}^n$ that is not in the $\text{Nul}(A).$
  \textbf{Show that if $\vec{v}$ is an eigenvector of $A^{T} A$ with eigenvalue $\lambda,$ there exists an eigenvector of $AA^{T}$ with eigenvalue $\lambda$.} Write out what the corresponding eigenvector of $A A^T$ sharing the same eigenvalue is in terms of defined variables.

  \ws{\vspace{100px}}

  \sol {
    Since $\vec{v}$ is an eigenvector of $A^{T} A,$ we know that $A^{T}A \vec{v} = \lambda \vec{v}.$ \vskip 1pt
    If we left multiply by $A$ we see that
    \begin{equation}
      A A^{T}A \vec{v} = A (\lambda \vec{v}) = \lambda (A \vec{v})
    \end{equation}
    We assumed that $\vec{v}$ is not in the $\text{Nul}(A)$ meaning $A \vec{v}$ will be nonzero.
    Therefore, we can say that $\vec{u} = A \vec{v}$ is an eigenvector of $AA^{T}$ with eigenvalue $\lambda.$
    Also, we note that this vector $\vec{u} = A \vec{v}$ will be a vector in the $\text{Col}(A).$
  }

  \qitem Let $\vec{u}$ be a nonzero vector in $\mathbb{R}^m$ that is not in the $\text{Nul}(A^{T}).$
  \textbf{Show that if $\vec{u}$ is an eigenvector of $AA^{T}$ with eigenvalue $\lambda,$ there must also be an eigenvector of $A^{T} A$ with eigenvalue $\lambda$.}

  \ws{\vspace{100px}}
  \sol {
    Since $\vec{u}$ is an eigenvector of $AA^{T},$ we know that $AA^{T} \vec{u} = \lambda \vec{u}.$ \vskip 1pt
    If we left multiply by $A^{T}$ we see that
    \begin{equation}
      A^{T}AA^{T} \vec{u} = A^{T} (\lambda \vec{u}) = \lambda (A^{T} \vec{u})
    \end{equation}
    We again assumed $\vec{u}$ is not in the $\text{Nul}(A^{T})$ meaning $A^{T} \vec{u}$ will be nonzero.
    Therefore, we can say that $\vec{v} = A^{T} \vec{u}$ is an eigenvector of $A^{T}A$ with eigenvalue $\lambda.$
    Again we note that this vector is in the $\text{Col}(A^{T}).$
  }
  \end{enumerate}

  \qitem We can pick $\vec{v}$ to be an eigenvector of $A^{T}A$ with unit norm. \textbf{Given this choice of $\vec{v}$, what is $\norm{A \vec{v}}$?}

  Aside: Why was it important to show that $A^T A$ has non-negative eigenvalues?

  \ws{\vspace{75px}}

  \sol {
    The Euclidean norm of a vector is the square root of its inner product with itself:
    \begin{equation}
      \norm{A \vec{v}} = \sqrt{(A \vec{v})^{T} (A \vec{v})} = \sqrt{\vec{v}^{T} A^{T} A \vec{v}} = \sqrt{\vec{v}^{T} \lambda \vec{v}} = \sqrt{\lambda \vec{v}^{T} \vec{v}} = \sqrt{\lambda \norm{v}} = \sqrt{\lambda}
    \end{equation}
    Note that we are allowed to take the square root and not end up with an imaginary answer since we showed that $\lambda\geq 0$ in the previous question in part (g).
  }

  \qitem Using the results from problem (c) and (d), \textbf{give a relationship between the unit eigenvector $\vec{v}$ of $A^T A$ with the corresponding unit eigenvector $\vec{u}$ of $A A^T$}, where they both share the same eigenvalue of their corresponding matrices.

  \ws{\vspace{75px}}
  \sol {
    We showed in part (c) that if $\vec{v}$ had unit norm, $\norm{A \vec{v}} = \sqrt{\lambda}.$ \vskip 1pt
    In part (a) we showed that there exists an eigenvector $\vec{w} = A \vec{v}$ of $A A^{T}.$
    However, we cannot assume this vector $\vec{w}$ has unit norm.
    Since $\vec{u}$ was chosen to be a unit vector in the direction of $\vec{w},$ we can say that $A \vec{v} = \sigma \vec{u}$ for some scalar $\sigma.$
    Taking the norm of both sides, we will see that:
    \begin{equation}
      \norm{A \vec{v}} = \norm{\sigma \vec{u}} = \sigma \norm{\vec{u}} = \sigma
    \end{equation}
    This shows that $\sigma = \sqrt{\lambda}$ so we see that $A \vec{v} = \sqrt{\lambda} \vec{u}.$
  }

  \qitem So far, we've been considering vectors outside the null spaces of $A$ and $A^T$. We want to ensure that the SVD still maps vectors in these null spaces to $\vec{0}$.

  \begin{enumerate}

  \qitem
  Let $\vec{v}$ be nonzero vector in $\mathbb{R}^{n}$ that is also in the null space of $A$.
  \textbf{Show that $\vec{v}$ is an eigenvector of $A^{T} A$ with eigenvalue $0.$}

  \meta {
    We're now going back to part (a) and considering the case in which $A \vec{v} = \vec{0}$ to show that vectors in the $\text{Nul}(A)$ are eigenvectors of $A^{T}A$ with eigenvalue $0.$
  }

  \ws{\vspace{75px}}

  \sol {
    If $\vec{v} \in \text{Nul}(A),$ then $A \vec{v} = \vec{0}.$ Left multiplying, by $A^{T}$ we see that:
    \begin{equation}
      A^{T} (A \vec{v}) = A^{T} \vec{0} = \vec{0} = 0 \cdot \vec{v}
    \end{equation}
    Therefore, $\vec{v}$ is an eigenvector of $A^{T} A$ with eigenvalue $0.$
  }

  \qitem Similarly, let $\vec{u}$ be nonzero vector in $\mathbb{R}^{m}$ that is also in the null space of $A^{T}.$
  \textbf{Show that $\vec{u}$ is an eigenvector of $AA^{T}$ with eigenvalue $0$.}

  \ws{\vspace{75px}}

  \sol {
    If $\vec{u} \in \text{Nul}(A^{T}),$ then $A^{T} \vec{u} = \vec{0}.$ Left multiplying, by $A$ we see that:
    \begin{equation}
      A (A^{T} \vec{u}) = A \vec{0} = \vec{0} = 0 \cdot \vec{u}
    \end{equation}
    Therefore, $\vec{u}$ is an eigenvector of $A A^{T}$ with eigenvalue $0$.
  }
  \end{enumerate}

  \newpage
  \qitem We will finally bring everything together using all of the facts proven above.

  If $A$ is a $m \times n$ matrix, of rank $r < n$, \textbf{show that $AV_{r} = U_{r} \Sigma_{r}.$}

  \begin{enumerate}
  \item $V_{r}$ is a $n \times r$ matrix of the first $r$ eigenvectors of $A^{T}A$ in $\text{Col}(A^{T})$
  \item $U_{r}$ is a $m \times r$ matrix of the first $r$ eigenvectors of $AA^{T}$ in $\text{Col}(A)$
  \item $\Sigma_{r}$ is a $r \times r$ diagonal matrix of the singular values $
  \sigma_1 = \sqrt{\lambda_{1}}, \dotsc, \sigma_r = \sqrt{\lambda_{r}}$, where the values are ordered from largest to smallest such that $\sigma_1 \geq \sigma_2 \geq \ldots \geq \sigma_r > 0$
  \end{enumerate}
  This form of the SVD is called the \textbf{compact SVD}.

  \textit{Hint 1: Try writing out the relationship between each $\vec{v}_{i}$ and $\vec{u}_{i}$ and convert these vector equations into a matrix equation.}

  \textit{Hint 2: Remember that $U_r, V_r$ contain an orthonormal set of vectors. What special properties do these matrices have?}

  \ws{\vspace{150px}}

  \sol {
    We've shown for eigenvectors of $A^{T} A, \ \vec{v}_{1}, \cdots, \vec{v_k} \in \text{Col}(A), \ A \vec{v}_{1} = \sigma_{1} \vec{u}_{1}, \cdots, A \vec{v_k} = \vec{u_k}.$ \\
    Therefore, if we try turning this into a matrix equation, we see that:
    $$A \begin{bmatrix} | & \ & | \\
    \vec{v}_{1} & \cdots & \vec{v}_{k}  \\ | & \ & | \end{bmatrix}
    =  \begin{bmatrix} | & \ & | \\ \sigma_{1} \vec{u}_{1} & \cdots & \sigma_{k} \vec{u}_{k} \\ | & \ & | \end{bmatrix}$$
    However, we can can notice that for $\vec{v}_{i},$
    $$A \vec{v}_{i} = 0 \cdot \vec{u}_{1} + \dotsc + \sigma_{i} \vec{u}_{i} + 0 \vec{u}_{i + 1} + \dotsc + 0 \vec{u}_{k}.$$
    For each individual vector, we get:
    $$A \vec{v}_{i} = U_{k} \begin{bmatrix} 0 \\ \vdots \\ \sigma_{i} \\ \vdots \\ 0 \end{bmatrix}$$
    Therefore, we get the relation:
    $$A \begin{bmatrix} | & \ & | \\
    \vec{v}_{1} & \cdots & \vec{v}_{k}  \\ | & \ & | \end{bmatrix} =
    \begin{bmatrix} | & \ & | \\
    \vec{u}_{1} & \cdots & \vec{u}_{k}  \\ | & \ & | \end{bmatrix}
    \begin{bmatrix} \sigma_{1} & \cdots & 0 \\
    \vdots & \ddots & \vdots \\
    0 & \cdots & \sigma_{k} \end{bmatrix}$$
  }

  \qitem \textbf{Now, derive the full SVD by completing the $U$ and $V$ matrices using the remaining eigenvectors of $A^{T}A$ and $AA^{T}$ that are in the $\text{Nul}(A)$ and $\text{Nul}(A^T)$, resulting in the matrix equation $A V = U \Sigma$.} You will have to consider both cases when $m \geq n$ and $m < n.$

  \ws{\vspace{200px}}

  \sol {
    We will consider the cases for when $m \geq n$ and $m < n.$ \\
    Suppose $m \geq n.$ \\
    For the remaining eigenvectors of $A^{T}A, \vec{v}_{k + 1}, \cdots, \vec{v}_{n} \in \text{Nul}(A), \ A \vec{v_{i}} = 0 \vec{u_{i}},$ up until $n.$ Therefore, we can try extending our matrix to all of the columns of $V$ with $\sigma_{k+1}, \dotsc, \sigma_{n} = 0.$\\
    $$A \begin{bmatrix} | & \ & | \\
    \vec{v}_{1} & \cdots & \vec{v}_{n}  \\ | & \ & | \end{bmatrix}
    =  \begin{bmatrix} | & \ & | \\ \sigma_{1} \vec{u}_{1} & \cdots & \sigma_{n} \vec{u}_{n} \\ | & \ & | \end{bmatrix} =
     \begin{bmatrix} | & \ & | \\ \vec{u}_{1} & \cdots & \vec{u}_{n} \\ | & \ & | \end{bmatrix} \begin{bmatrix} \sigma_{1} & 0 &  \cdots & 0 \\ 0 & \sigma_{2} & \cdots & 0 \\ \vdots & \vdots & \ddots & \sigma_{n} \\
    \vdots & \vdots & \ddots & 0 \\ 0 & 0 & 0 & 0 \end{bmatrix} = U \Sigma$$
    Now let's suppose $m < n.$ \\
    We start off with the eigenvectors of $A^{T} A$ again: $A \vec{v}_{1} = \sigma_{1} \vec{u}_{1}, \cdots, A \vec{v}_{k} = \vec{u}_{k}.$ \\
    We will get the same matrix equation:
    $$A \begin{bmatrix} | & \ & | \\
    \vec{v}_{1} & \cdots & \vec{v}_{k}  \\ | & \ & | \end{bmatrix}
    =  \begin{bmatrix} | & \ & | \\ \sigma_{1} \vec{u}_{1} & \cdots & \sigma_{k} \vec{u}_{k} \\ | & \ & | \end{bmatrix}$$
    For the remaining eigenvectors of $A^{T}A, \vec{v}_{k + 1}, \cdots, \vec{v}_{n} \in \text{Nul}(A), \ A \vec{v_{i}} = 0 \vec{u_{i}},$ up until $m < n.$ Notice that we cannot cover all eigenvectors of $A^{T} A$ but we will cover up to $m < n.$
    $$A \begin{bmatrix} | & \ & | \\
    \vec{v}_{1} & \cdots & \vec{v}_{m}  \\ | & \ & | \end{bmatrix} =
    \begin{bmatrix} | & \ & | \\ \sigma_{1} \vec{u}_{1} & \cdots & \sigma_{m} \vec{u}_{m} \\ | & \ & | \end{bmatrix} =
     \begin{bmatrix} | & \ & | \\ \vec{u}_{1} & \cdots & \vec{u}_{m} \\ | & \ & | \end{bmatrix} \begin{bmatrix} \sigma_{1} & 0 &  \cdots & 0 \\ 0 & \sigma_{2} & \cdots & 0 \\ \vdots & \vdots & \ddots & \sigma_{m} \end{bmatrix}$$
    We however, can still extend to $V$ since $A \vec{v}_{i} = \vec{0}$ for $i \geq n.$ We will do this by padding $0$'s to the right of the $\Sigma$ matrix.
    $$A \begin{bmatrix} | & \ & | \\
    \vec{v}_{1} & \cdots & \vec{v}_{n}  \\ | & \ & | \end{bmatrix} =
    \begin{bmatrix} | & \ & | &  & | \\ \sigma_{1} \vec{u}_{1} & \cdots & \sigma_{m} \vec{u}_{m} & \cdots & \vec{0} \\ | & \ & | &  & | \end{bmatrix} =
     \begin{bmatrix} | & \ & | \\ \vec{u}_{1} & \cdots & \vec{u}_{m} \\ | & \ & | \end{bmatrix} \begin{bmatrix} \sigma_{1} & 0 & \cdots & 0 & \cdots & 0 \\ 0 & \sigma_{2} & \cdots & 0 & \cdots & 0 \\ \vdots & \vdots & \ddots & \sigma_{m} & \cdots&  0 \end{bmatrix}$$
    In both cases, we see that $AV = U\Sigma .$
  }

  \qitem \textbf{Why does the above equation imply that $A = U \Sigma V^{T}?$} This is the \textbf{full SVD.}

  \ws{\vspace{50px}}

  \sol {
    Since $V$ is an orthonormal matrix, we can right multiply by $V^{-1} = V^{T}$ to get:
    \begin{equation}
      A = U \Sigma V^{T}
    \end{equation}
  }

  \newpage

  \qitem \textbf{OPTIONAL:} The Singular Value Decomposition will compute the eigenvectors of $A^{T} A$ and $AA^{T}.$
  Based on the previous parts, we can make the realization that each of these eigenvectors are part of one of the four fundamental subspaces of $A: \text{Col}(A), \text{Nul}(A), \text{Col}(A^{T}), \text{Nul}(A^{T}).$
  \textbf{Show which eigenvectors of $A^{T} A$ and $AA^{T}$ belong to which subspaces of $\mathbb{R}^{n}$ and $\mathbb{R}^{m}.$}

  \ws{\vspace{100px}}

  \sol {

    The matrix $AA^{T}$ is an $m \times m$ matrix. \\
    We showed in part (a) that there are eigenvectors $\vec{u}$ of $AA^{T}$ that are in $\text{Col}(A).$ \\
    We also showed in part (a) that there are eigenvectors $\vec{u}$ of $AA^{T}$ with eigenvalue $0,$ that are in $\text{Nul}(A^{T}).$

    The matrix $A^{T}A$ is an $n \times n$ matrix. \\
    We showed in part (g) that there are eigenvectors $\vec{v}$ of $A^{T}A$ that are in $\text{Col}(A^{T}).$ \\
    We also showed in part (e) that there are eigenvectors $\vec{v}$ of $A^{T}A$ with eigenvalue $0,$ that are in $\text{Nul}(A).$
  }

  \qitem \textbf{OPTIONAL, BUT RECOMMENDED:} The Fundamental Theorem of Linear Algebra states that:
  \begin{align*}
    \text{Col}(A) &\perp \text{Nul}(A^{T}) \\
    \text{Col}(A^{T}) &\perp \text{Nul}(A)
  \end{align*}
  \textbf{Show that this statement can be expressed using the eigenvectors of $A^{T}A$ and $AA^{T},$ and that they are in fact basis vectors for these fundamental subspaces.}

  \ws{\vspace{100px}}

  \sol {
    We know from the Spectral Theorem, that $A^{T}A$ has $n$ linearly independent eigenvectors.
    This means that the vectors $\vec{v}_{1}, \dotsc, \vec{v}_{n}$ will form a basis for $\mathbb{R}^{n}.$
    We also know from the Rank-Nullity theorem that $\text{dim} \ \text{Col}(A) + \text{dim} \ \text{Nul}(A) = n.$
    Using the fact that $\text{Rank}(A) = \text{Rank}(A^{T}),$ we see that $\text{dim} \ \text{Col}(A^{T}) + \text{dim} \ \text{Nul}(A) = n.$

    We also saw in the previous parts, that if $A$ has rank $k < n,$ eigenvectors $\vec{v}_{1}, \dotsc, \vec{v}_{k}$ will all be in the $\text{Col}(A^{T})$ while $\vec{v}_{k+1}, \dotsc, \vec{v}_{n}$ will be in the $\text{Nul}(A).$ Therefore, we conclude by saying that $\vec{v}_{1}, \dotsc, \vec{v}_{k}$ forms a basis for  $\text{Col}(A^{T})$ and $\vec{v}_{k+1}, \dotsc, \vec{v}_{n}$ will form a basis for $\text{Nul}(A).$

    Using these basis vectors, we can write any vector $\vec{x}$ in $\text{Col}(A^{T})$ as $\vec{x} = \alpha_{1} \vec{v}_{1} + \dotsc + \alpha_{k} \vec{v}_{k}$ and any vector $\vec{y}$ in $\text{Nul}(A)$ can be written as $\vec{y} = \alpha_{k+1} \vec{v}_{k+1} + \dotsc + \alpha_{n} \vec{v}_{n}.$ $\vec{x}$ will be orthogonal to $\vec{y}$ since the Spectral Theorem says that we can pick eigenvectors $\vec{v}_{i}$ of $A^{T}A$ are all mutually orthogonal.
    Therefore, the $\text{Col}(A^{T})$ will be orthogonal to the $\text{Nul}(A).$

    The exact same argument can be made for the eigenvectors of $AA^{T}$ by creating bases for the $\text{Col}(A)$ and $\text{Nul}(A^{T}).$ We again can invoke the Rank-Nullity theorem and the fact that these eigenvectors are orthogonal to conclude that these two subspaces are orthogonal to each other.
  }

\end{enumerate}
